[TRAINING]
STAGING_BUCKET = gs://gm-llm-training/gm-models
INPUT_DIR = /home/gimarchetti/mnt/llm-training/gm-models/datasets/dolma_ds
VOCAB_FILE = ./c4vocab.pkl
BATCH_SIZE_PER_GPU = 1
EPOCHS = 100

[MODEL]
VOCAB_SIZE = 50000
EMBEDDING_DIM = 2048
N_HEADS = 16
N_KV_HEADS = 16
NUM_BLOCKS = 16
FEED_FORWARD_DIM = 5120

[DATA]
VALIDATION_SPLIT = 0.2
MIN_STRING_LEN = 1536
SEQ_LEN = 2048

[DISTRIBUTION]
DATA_DIM = 16
MODEL_DIM = 4
